{% extends "layout.html" %}
{% from "visualization.html" import agent_visualization %}

{% block title %}PPO+LSTM Implementation Demo{% endblock %}

{% block content %}
<article class="bg-white shadow rounded-lg p-8 mb-8">
    <h1 class="text-3xl font-bold mb-6">PPO+LSTM Implementation & Experiments</h1>
    
    <div class="bg-gray-50 border-l-4 border-blue-500 p-4 mb-8">
        <p class="text-lg"><strong>TLDR:</strong> I implemented PPO+LSTM from scratch and got it to solve three increasingly complex tasks: cartpole without velocity information, a memory-based navigation task, and finally a partially observable maze where the agent can only see a 3x3 area around itself, achieving an 80% success rate on 13x13 mazes with time limit 256 steps.</p>
    </div>

    <section class="mb-12">
        <h2 class="text-2xl font-bold mb-4">History</h2>
        <p class="text-gray-600 mb-6">On April 13th, 2019, OpenAI's Dota 2 AI system, OpenAI Five, became the first AI to beat a world champion team in an esports game. The algorithm behind this was proximal policy optimization(PPO) with LSTMs, which, given enough compute (a few thousand years of self-play), enabled it to achieve superhuman performance.</p>
        
        <p class="text-gray-600 mb-6">Inspired by this, I decided to implement it too. I've linked some blogs at the end of the post if you are interested in this.</p>
        
        <div class="bg-red-100 border-l-4 border-red-500 p-4 mb-6">
            <p class="text-red-700">TODO: You can play all the games below on this Google Colab (in the browser). You can also try some below, but the server latency varies.</p>
        </div>
        
        <p class="text-gray-600 mb-6">All the code for this project can be found in <a href="https://github.com/RaresFelix/ppo_lstm" class="text-blue-600 hover:underline">this GitHub repo</a>.</p>
    </section>

    <section class="mb-12">
        <h2 class="text-2xl font-bold mb-4">Architecture</h2>
        
        <!-- <div class="flex justify-center mb-6">
            <img src="/ppo_lstm_viewer/static/images/AgentEnvLoop1.png" alt="Agent-Environment Interaction Loop" class="max-w-full h-auto">
        </div> -->
        
        <p class="text-gray-600 mb-6">For each environment, the agent receives an observation in the form of an array of numbers, and has to take an action.</p>
        
        <!-- <div class="bg-red-100 border-l-4 border-red-500 p-4 mb-6">
            <p class="text-red-700">TODO: Add image of GUI vs agent observation</p>
        </div> -->

        <p class="text-gray-600 mb-6">For the agent to be able to form long-term connections, I used an LSTM (long short-term memory). It's a type of recurrent neural network that can handle longer term dependencies well. Chris Olah has a very good article on how LSTMs work <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs" class="text-blue-600 hover:underline">here</a>.</p>

        <div class="bg-red-100 border-l-4 border-red-500 p-4 mb-6">
            <p class="text-red-700">TODO: Add image of observation + lstm -> hidden states / features</p>
        </div>

        <p class="text-gray-600 mb-6">These hidden states / features act as memory for the agent.</p>

        <!-- <div class="bg-red-100 border-l-4 border-red-500 p-4 mb-6">
            <p class="text-red-700">TODO: Add image of features going into agent heads</p>
        </div> -->

        <p class="text-gray-600 mb-6">These can be used by the actor head to predict the best action, or by the critic head to predict "how good" the current state is (this is very useful during training).</p>
    </section>

    <section class="mb-12">
        <h2 class="text-2xl font-bold mb-4">Example 1: Cartpole without velocities</h2>
        
        <p class="text-gray-600 mb-6">Cartpole is a classic reinforcement learning environment. The agent can see the position, angle, speed and angular velocity of a pole, and has to balance it for as long as possible.</p>
        
        <p class="text-gray-600 mb-6">You can play it yourself below. The interesting thing about it is that the human brain doesn't receive the velocities of the pole either. Using recurrent biological circuitry, your brain finds the velocities and approximates an algorithm for solving the problem.</p>

        <div class="bg-red-100 border-l-4 border-red-500 p-4 mb-6">
            <p class="text-red-700">TODO: Add Weights & Biases training graph</p>
        </div>
        

        <div class="bg-red-100 border-l-4 border-red-500 p-4 mb-6">
            <p class="text-red-700">TODO: Add visualization of Cartpole without speed, memory heatmap, and playable demo</p>
        </div>
    </section>

    <section class="mb-12">
        <h2 class="text-2xl font-bold mb-4">Example 2: Minigrid Memory</h2>
        
        <p class="text-gray-600 mb-6">This is a demonstration of longer-term memory capabilities. The agent can see only the 3 x 3 area around him, and has to remember which object to select at the end of the hallway. You can see it's memory in time below.</p>

        <div class="bg-gray-50 p-6 rounded-lg">
            {{ agent_visualization("memory", title="Memory Task Analysis", left_width=400, right_width=250) }}
        </div>

        <iframe src="https://api.wandb.ai/links/raresfelix/zods561s" style="border:none;height:1024px;width:100%"></iframe>
    </section>

    <section class="mb-12">
        <h2 class="text-2xl font-bold mb-4">Example 3: Minigrid Custom Maze</h2>
        
        <p class="text-gray-600 mb-6">This is a custom minigrid env I wrote for this project. The mazes are generated with Kruskal's algorithm.</p>

        <!-- Agent Analysis subsection -->
        <div class="mb-8">
            <h3 class="text-xl font-bold mb-4">Agent Analysis</h3>
            <p class="text-gray-600 mb-6">Visualize the agent's memory and decision-making process across different runs.</p>
            
            <div class="bg-gray-50 p-6 rounded-lg">
                {{ agent_visualization("maze", title="Maze Navigation Analysis") }}
            </div>
        </div>
    </section>
</article>

<style>
    @media (min-width: 1024px) {
        .connecting-line {
            display: flex !important;
        }
    }
    @media (max-width: 1023px) {
        .connecting-line {
            display: none !important;
        }
    }
</style>

<script src="{{ url_for('static', filename='js/visualization.js') }}"></script>
<script>
document.addEventListener('DOMContentLoaded', function() {
    new AgentVisualization('memory');
    new AgentVisualization('maze');
});
</script>
{% endblock %}
